{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. When a vessel arrives at a port, these agents must take actions by transfering a certain amount of containers to / from the vessel. The objective is for the agents to learn policies that minimize the overall container shortage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Common info\n",
    "PORT_ATTRIBUTES = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "VESSEL_ATTRIBUTES = [\"empty\", \"full\", \"remaining_space\"]\n",
    "ACTION_SPACE = list(np.linspace(-1.0, 1.0, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType\n",
    "\n",
    "def get_state(decision_event, snapshots, look_back=7):\n",
    "    \"\"\"\n",
    "    This function converts environment observations to state vectors that encode temporal and spatial\n",
    "    information. The temporal information includes relevant port and vessel information, such as shortage\n",
    "    and remaining space, over the past k days (here k = 7). The spatial information includes features\n",
    "    of the downstream ports.\n",
    "    \"\"\"\n",
    "    tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "    ticks = [tick - rt for rt in range(look_back - 1)]\n",
    "    future_port_idx_list = snapshots[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "    port_features = snapshots[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): PORT_ATTRIBUTES]\n",
    "    vessel_features = snapshots[\"vessels\"][tick: vessel_idx: VESSEL_ATTRIBUTES]\n",
    "    return np.concatenate((port_features, vessel_features))\n",
    "\n",
    "def get_env_action(model_action, decision_event, vessel_snapshots):\n",
    "    \"\"\"\n",
    "    This function converts agents' output (an integer that maps to a percentage of containers to be loaded\n",
    "    to or unloaded from the vessel) to action objects that can be executed by the environment.\n",
    "    \"\"\"\n",
    "    scope = decision_event.action_scope\n",
    "    tick = decision_event.tick\n",
    "    port = decision_event.port_idx\n",
    "    vessel = decision_event.vessel_idx\n",
    "    zero_action_idx = len(ACTION_SPACE) / 2  # index corresponding to value zero.\n",
    "\n",
    "    vessel_space = vessel_snapshots[tick:vessel:VESSEL_ATTRIBUTES][2]\n",
    "    early_discharge = vessel_snapshots[tick:vessel:\"early_discharge\"][0]\n",
    "    percent = abs(ACTION_SPACE[model_action])\n",
    "    if model_action < zero_action_idx:\n",
    "        action_type = ActionType.LOAD\n",
    "        actual_action = min(round(percent * scope.load), vessel_space)\n",
    "    elif model_action > zero_action_idx:\n",
    "        action_type = ActionType.DISCHARGE\n",
    "        plan_action = percent * (scope.discharge + early_discharge) - early_discharge\n",
    "        actual_action = round(plan_action) if plan_action > 0 else round(percent * scope.discharge)\n",
    "    else:\n",
    "        actual_action, action_type = 0, None\n",
    "\n",
    "    return Action(vessel, port, actual_action, action_type)\n",
    "\n",
    "def get_reward(\n",
    "    decision_event, port_snapshots, reward_time_window=100, time_decay=0.97,\n",
    "    fulfillment_factor=1.0, shortage_factor=1.0    \n",
    "):\n",
    "    \"\"\"\n",
    "    This function computes the reward of a given action as a linear combination of fulfillment and\n",
    "    shortage within a future time frame (set to 100 here).\n",
    "    \"\"\"\n",
    "    start_tick = decision_event.tick + 1\n",
    "    end_tick = decision_event.tick + reward_time_window\n",
    "    ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "    future_fulfillment = port_snapshots[ticks::\"fulfillment\"]\n",
    "    future_shortage = port_snapshots[ticks::\"shortage\"]\n",
    "    decay_list = [\n",
    "        time_decay ** i for i in range(end_tick - start_tick)\n",
    "        for _ in range(future_fulfillment.shape[0] // (end_tick - start_tick))\n",
    "    ]\n",
    "\n",
    "    tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "    tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "    return np.float32(fulfillment_factor * tot_fulfillment - shortage_factor * tot_shortage)\n",
    "\n",
    "def get_training_data(trajectory, port_snapshots):\n",
    "    \"\"\"\n",
    "    This function processes a trajectory of transitions into training data. The transitions are\n",
    "    bucketed by the agent ID.\n",
    "    \"\"\"\n",
    "    agent_ids = trajectory[\"agent_id\"]\n",
    "    events = trajectory[\"event\"]\n",
    "    states = trajectory[\"state\"]\n",
    "    actions = trajectory[\"action\"]\n",
    "    log_p = trajectory[\"log_p\"]\n",
    "\n",
    "    training_data = defaultdict(lambda: defaultdict(list))\n",
    "    for i in range(len(states)):\n",
    "        data = training_data[agent_ids[i]]\n",
    "        data[\"state\"].append(states[i])\n",
    "        data[\"action\"].append(actions[i])\n",
    "        data[\"log_p\"].append(log_p[i])\n",
    "        data[\"reward\"].append(get_reward(events[i], port_snapshots))\n",
    "        \n",
    "    for agent_id in training_data:\n",
    "        for key, vals in training_data[agent_id].items():\n",
    "            training_data[agent_id][key] = np.asarray(vals, dtype=np.float32 if key == \"reward\" else None)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "The out-of-the-box ActorCritic is used as our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl import ActorCritic, ActorCriticConfig, FullyConnectedBlock, OptimOption, SimpleMultiHeadModel\n",
    "\n",
    "# We consider the port in question as well as two downstream ports, hence the factor 3.\n",
    "# We consider the states of these ports over the past 7 days plus the current day, hence the factor 8.\n",
    "input_dim = 3 * 8 * len(PORT_ATTRIBUTES) + len(VESSEL_ATTRIBUTES)\n",
    "agent_config = {\n",
    "    \"model\": {\n",
    "        \"actor\": {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": len(ACTION_SPACE),\n",
    "            \"hidden_dims\": [256, 128, 64],\n",
    "            \"activation\": nn.Tanh,\n",
    "            \"softmax\": True,\n",
    "            \"batch_norm\": False,\n",
    "            \"head\": True\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": 1,\n",
    "            \"hidden_dims\": [256, 128, 64],\n",
    "            \"activation\": nn.LeakyReLU,\n",
    "            \"softmax\": False,\n",
    "            \"batch_norm\": True,\n",
    "            \"head\": True\n",
    "        }\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"actor\": OptimOption(optim_cls=Adam, optim_params={\"lr\": 0.001}),\n",
    "        \"critic\": OptimOption(optim_cls=RMSprop, optim_params={\"lr\": 0.001})\n",
    "    },\n",
    "    \"hyper_params\": {\n",
    "        \"reward_discount\": .0,\n",
    "        \"critic_loss_func\": nn.SmoothL1Loss(),\n",
    "        \"train_iters\": 10,\n",
    "        \"actor_loss_coefficient\": 0.1,  # loss = actor_loss_coefficient * actor_loss + critic_loss\n",
    "        \"k\": 1,  # for k-step return\n",
    "        \"lam\": 0.0  # lambda return coefficient\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_ac_agent():\n",
    "    actor_net = FullyConnectedBlock(**agent_config[\"model\"][\"actor\"])\n",
    "    critic_net = FullyConnectedBlock(**agent_config[\"model\"][\"critic\"])\n",
    "    ac_model = SimpleMultiHeadModel(\n",
    "        {\"actor\": actor_net, \"critic\": critic_net}, optim_option=agent_config[\"optimization\"],\n",
    "    )\n",
    "    return ActorCritic(ac_model, ActorCriticConfig(**agent_config[\"hyper_params\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor (Roll-out Loop)\n",
    "\n",
    "Below is an implementation of the roll-out loop. Note how the shaping functions are used during the agents' interaction with the environment. For each transition, we record the agent ID, event, state, action and its log probability. At the end of the roll-out, the recorded sequence of transitions (the trajectory) gets processed into training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsActor\n",
    "\n",
    "\n",
    "class BasicActor(AbsActor):\n",
    "    def roll_out(self, index, training=True):\n",
    "        self.env.reset()\n",
    "        trajectory = {key: [] for key in [\"state\", \"action\", \"agent_id\", \"event\", \"log_p\"]}\n",
    "        metrics, event, is_done = self.env.step(None)\n",
    "        while not is_done:\n",
    "            state = get_state(event, self.env.snapshot_list)\n",
    "            agent_id = event.port_idx\n",
    "            action, log_p = self.agent[agent_id].choose_action(state)\n",
    "            trajectory[\"state\"].append(state)\n",
    "            trajectory[\"agent_id\"].append(agent_id)\n",
    "            trajectory[\"event\"].append(event)\n",
    "            trajectory[\"action\"].append(action)\n",
    "            trajectory[\"log_p\"].append(log_p)\n",
    "            env_action = get_env_action(action, event, self.env.snapshot_list[\"vessels\"])\n",
    "            metrics, event, is_done = self.env.step(env_action)\n",
    "\n",
    "        return get_training_data(trajectory, self.env.snapshot_list[\"ports\"]) if training else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This code cell demonstrates a typical single-threaded training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep-0: {'order_requirements': 2240000, 'container_shortage': 1422736, 'operation_number': 4220466}\n",
      "ep-1: {'order_requirements': 2240000, 'container_shortage': 1330641, 'operation_number': 3919970}\n",
      "ep-2: {'order_requirements': 2240000, 'container_shortage': 996878, 'operation_number': 3226186}\n",
      "ep-3: {'order_requirements': 2240000, 'container_shortage': 703662, 'operation_number': 3608511}\n",
      "ep-4: {'order_requirements': 2240000, 'container_shortage': 601934, 'operation_number': 3579281}\n",
      "ep-5: {'order_requirements': 2240000, 'container_shortage': 629344, 'operation_number': 3456707}\n",
      "ep-6: {'order_requirements': 2240000, 'container_shortage': 560709, 'operation_number': 3511869}\n",
      "ep-7: {'order_requirements': 2240000, 'container_shortage': 483549, 'operation_number': 3613713}\n",
      "ep-8: {'order_requirements': 2240000, 'container_shortage': 390332, 'operation_number': 3817820}\n",
      "ep-9: {'order_requirements': 2240000, 'container_shortage': 361151, 'operation_number': 3823994}\n",
      "ep-10: {'order_requirements': 2240000, 'container_shortage': 442086, 'operation_number': 3647343}\n",
      "ep-11: {'order_requirements': 2240000, 'container_shortage': 390846, 'operation_number': 3784078}\n",
      "ep-12: {'order_requirements': 2240000, 'container_shortage': 309105, 'operation_number': 3896184}\n",
      "ep-13: {'order_requirements': 2240000, 'container_shortage': 430801, 'operation_number': 3787247}\n",
      "ep-14: {'order_requirements': 2240000, 'container_shortage': 368042, 'operation_number': 3793428}\n",
      "ep-15: {'order_requirements': 2240000, 'container_shortage': 383015, 'operation_number': 3829184}\n",
      "ep-16: {'order_requirements': 2240000, 'container_shortage': 373584, 'operation_number': 3772635}\n",
      "ep-17: {'order_requirements': 2240000, 'container_shortage': 411397, 'operation_number': 3644350}\n",
      "ep-18: {'order_requirements': 2240000, 'container_shortage': 307861, 'operation_number': 3842550}\n",
      "ep-19: {'order_requirements': 2240000, 'container_shortage': 324650, 'operation_number': 3848202}\n",
      "ep-20: {'order_requirements': 2240000, 'container_shortage': 367267, 'operation_number': 3739414}\n",
      "ep-21: {'order_requirements': 2240000, 'container_shortage': 326153, 'operation_number': 3822407}\n",
      "ep-22: {'order_requirements': 2240000, 'container_shortage': 466237, 'operation_number': 3516845}\n",
      "ep-23: {'order_requirements': 2240000, 'container_shortage': 429538, 'operation_number': 3603386}\n",
      "ep-24: {'order_requirements': 2240000, 'container_shortage': 241307, 'operation_number': 3986364}\n",
      "ep-25: {'order_requirements': 2240000, 'container_shortage': 260224, 'operation_number': 3971519}\n",
      "ep-26: {'order_requirements': 2240000, 'container_shortage': 190507, 'operation_number': 4060439}\n",
      "ep-27: {'order_requirements': 2240000, 'container_shortage': 152822, 'operation_number': 4146195}\n",
      "ep-28: {'order_requirements': 2240000, 'container_shortage': 91878, 'operation_number': 4300404}\n",
      "ep-29: {'order_requirements': 2240000, 'container_shortage': 78752, 'operation_number': 4297044}\n",
      "ep-30: {'order_requirements': 2240000, 'container_shortage': 202098, 'operation_number': 4047921}\n",
      "ep-31: {'order_requirements': 2240000, 'container_shortage': 161871, 'operation_number': 4113281}\n",
      "ep-32: {'order_requirements': 2240000, 'container_shortage': 74649, 'operation_number': 4311775}\n",
      "ep-33: {'order_requirements': 2240000, 'container_shortage': 54402, 'operation_number': 4330703}\n",
      "ep-34: {'order_requirements': 2240000, 'container_shortage': 42802, 'operation_number': 4353353}\n",
      "ep-35: {'order_requirements': 2240000, 'container_shortage': 49236, 'operation_number': 4346898}\n",
      "ep-36: {'order_requirements': 2240000, 'container_shortage': 74055, 'operation_number': 4280054}\n",
      "ep-37: {'order_requirements': 2240000, 'container_shortage': 66899, 'operation_number': 4312042}\n",
      "ep-38: {'order_requirements': 2240000, 'container_shortage': 29641, 'operation_number': 4385481}\n",
      "ep-39: {'order_requirements': 2240000, 'container_shortage': 56018, 'operation_number': 4354815}\n",
      "ep-40: {'order_requirements': 2240000, 'container_shortage': 60899, 'operation_number': 4361779}\n",
      "ep-41: {'order_requirements': 2240000, 'container_shortage': 173904, 'operation_number': 4122191}\n",
      "ep-42: {'order_requirements': 2240000, 'container_shortage': 247550, 'operation_number': 3974436}\n",
      "ep-43: {'order_requirements': 2240000, 'container_shortage': 63954, 'operation_number': 4343888}\n",
      "ep-44: {'order_requirements': 2240000, 'container_shortage': 135500, 'operation_number': 4209077}\n",
      "ep-45: {'order_requirements': 2240000, 'container_shortage': 33277, 'operation_number': 4396770}\n",
      "ep-46: {'order_requirements': 2240000, 'container_shortage': 206337, 'operation_number': 4036853}\n",
      "ep-47: {'order_requirements': 2240000, 'container_shortage': 45787, 'operation_number': 4392605}\n",
      "ep-48: {'order_requirements': 2240000, 'container_shortage': 51500, 'operation_number': 4355021}\n",
      "ep-49: {'order_requirements': 2240000, 'container_shortage': 76485, 'operation_number': 4298993}\n"
     ]
    }
   ],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import MultiAgentWrapper\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "set_seeds(1024)  # for reproducibility\n",
    "\n",
    "# Step 1: create a CIM environment for a toy dataset\n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "# Step 2: create agents\n",
    "agent = MultiAgentWrapper({name: get_ac_agent() for name in env.agent_idx_list})\n",
    "# Step 3: training loop\n",
    "actor = BasicActor(env, agent)\n",
    "for ep in range(50):\n",
    "    exp_by_agent = actor.roll_out(ep)\n",
    "    print(f\"ep-{ep}: {env.metrics}\")\n",
    "    for agent_id, exp in exp_by_agent.items():\n",
    "        agent[agent_id].learn(exp[\"state\"], exp[\"action\"], exp[\"log_p\"], exp[\"reward\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maro",
   "language": "python",
   "name": "maro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
